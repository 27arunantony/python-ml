{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study: Support Vector Machines in Object Detection\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What we will discuss today..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Basic Introduction of Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  #### a. What is SVM ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  #### b. Applications of SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Case Study: Digit Recognition in Images\n",
    "  \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  ####   a. Object Detection Problem?      \n",
    "\n",
    "- ####   b. How to prepare dataset?      \n",
    "\n",
    "- ####   c. What kind of features required?  \n",
    "\n",
    "- ####   d. Histogram of oriented gradient as the features.   \n",
    "\n",
    "- #### e. Prepare feature set for traiining our model \n",
    "\n",
    "- #### f. Train our SVM model to classify digits in the Images.\n",
    "\n",
    "- #### g. Test the classifier on images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Support Vector Machine?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machines are perhaps one of the most popular machine learning algorithms; it is a supervised machine learning algorithm which can be used for both classification and regression tasks. However, it is mostly used in classification problems. SVMs are also known as; Maximal margin classifier, Soft margin classifier, linear SVM and kernel based SVM. \n",
    "\n",
    "## What it does actually?\n",
    "\n",
    "Support Vectors are simply the co-ordinates of individual observation. Let’s understand it with the help of an example.\n",
    " We have a population composed of 50%-50% Males and Females. Using a sample of this population, you want to create some set of rules which will guide us the gender class for rest of the population. Using this algorithm, we intend to build a robot which can identify whether a person is a Male or a Female. This is a sample problem of classification analysis. Using some set of rules, we will try to classify the population into two possible segments. For simplicity, let’s assume that the two differentiating factors identified are; height of the individual and hair Length. Following is a scatter plot of the sample.\n",
    "\n",
    "<img src=\"SVM_2.png\">\n",
    "\n",
    "Now as I have mentioned earlier that SVM are the coordinates of individual observations; for instance, (45,150) is a support vector which corresponds to a female. Support Vector Machine is a frontier which best segregates the Male from the Females. In this case, the two classes are well separated from each other; hence it is easier to find a SVM.\n",
    "\n",
    "Now question is how to find out the frontiers; for current example, following figure shows three possible frontiers;\n",
    "\n",
    "<img src=\"SVM_3.png\">\n",
    "\n",
    "So what do you think; How do we decide which is the best frontier for this particular problem statement?\n",
    "The easiest way to interpret the objective function in a SVM is to find the minimum distance of the frontier from closest support vector (this can belong to any class). For instance, orange frontier is closest to blue circles. And the closest blue circle is 2 units away from the frontier. Once we have these distances for all the frontiers, we simply choose the frontier with the maximum distance (from the closest support vector). Out of the three shown frontiers, we see the black frontier is farthest from nearest support vector (i.e. 15 units).\n",
    "\n",
    "What is Hyper-plane?\n",
    "Geometry tells us that a hyperplane is a subspace of one dimension less than its ambient space. For instance, a hyperplane of an n-dimensional space is a flat subset with dimension n − 1. By its nature, it separates the space into two half spaces. For machine learning tasks we can re write the hyper-planes as;\n",
    "- Linear decision surface that splits the space into two parts.\n",
    "- Hyper-plane is a Binary classifier.\n",
    "Following figure shows the hyper planes;\n",
    "\n",
    "<img src=\"Hyper-Planes.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application of SVMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Image Segmentation and Categorization\n",
    "2. Geographic Image Processing\n",
    "3. Handwriting recognition\n",
    "4. Healthcare : Analyzing a group of over million people for myocardial infarction within a period of 10 years is an application area of Support vector machines.\n",
    "5. Prediction whether a person is depressed or not based on bag of words from the corpus seems to be conveniently solvable using SVM.\n",
    "\n",
    "# Case Study: Object Detection and Classification using SVM\n",
    "\n",
    "## What is an Image ?\n",
    "\n",
    "An image is just another numerical matrix which you have seen in your maths classes earlier; you can apply any algebric operation on it as you can apply on any other matrix; these operations may be simple maths such as addition, subtraction, multiplication etc. or it may be any complex analysis such as singular vector decomposition or principle component analysis. we can represent an image as following:\n",
    "\n",
    "<img src=\"lincoln_pixel_values.png\">\n",
    "\n",
    "## MNIST Digit Recognition Problem in Image processing\n",
    "\n",
    "This problem is known as the \"Hello World!\" problem of Machine learning world; whether you are working with conventional ML algorithms like one we are working on or stat of the art deep learning every thing related to classification starts from here, reason for this love is it is simple to understand so we will also use the same application to start our journey with it.\n",
    "\n",
    "Lets see how our the data set looks like and what are the expectations with the classifiers.\n",
    "\n",
    "<img src=\"DigitData.png\">\n",
    "\n",
    "### What does we wanna do?\n",
    "Well we want to build a classifier which can do this..\n",
    "\n",
    "<img src=\"DigitRecognition.png\">\n",
    "\n",
    "\n",
    "### So How the hell we gonna do that!!!\n",
    "\n",
    "Well answer is quite simple for that, we will train a Classifier to this work for us!\n",
    "\n",
    "But before moving forward lets see how we can deal with this problem using a Machine learning based frame work\n",
    "\n",
    "\n",
    "## Generalize Machine Learning framework\n",
    "<img src=\"GeneralizedML.png\">\n",
    "\n",
    "We will start our journey with data preparation\n",
    "\n",
    "# Data Preparation\n",
    "\n",
    "Well as this is most published and well appreciated problem, we can easily download its data set from online repositary; original data set is quite huge in size, it have around 60000 hand written digits from different people. but we will not going to work with all 60k we will load only 5000 data instances and try to come up with a solution.\n",
    "\n",
    "lets write some code to fetch the data set and see how it really looks like;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_digits(datasetPath):\n",
    "    # build the dataset and then split it into data\n",
    "    # and labels\n",
    "    data = np.genfromtxt(datasetPath, delimiter = \",\", dtype = \"uint8\")\n",
    "    target = data[:, 0]\n",
    "    data = data[:, 1:].reshape(data.shape[0], 28, 28)\n",
    "    print(type(data))\n",
    "    print(data[0])\n",
    "\n",
    "    # return a tuple of the data and targets\n",
    "    return (data, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets plot some of the loaded instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data,label = load_digits('digits.csv')\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(9):\n",
    "  plt.subplot(3,3,i+1)\n",
    "  plt.tight_layout()\n",
    "  plt.imshow(data[i], cmap='gray', interpolation='none')\n",
    "  plt.title(\"Digit: {}\".format(label[i]))\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this is how our data set look like; now what to do with this data set; well this problem is not that simple as it is looking. Before starting to implement a classifier we need to put a lot of work to pre-process the data so that we can make a efficient classifier we will do following pre-processing operations to our data set.\n",
    "\n",
    "- a. Deskew images.\n",
    "- b. Re-Center image content.\n",
    "\n",
    "We will write following line of code for deskew an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-bbdcb499bf9f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdeskew\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;31m# grab the width and height of the image and compute\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m# moments for the image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "def deskew(image, width):\n",
    "    # grab the width and height of the image and compute\n",
    "    # moments for the image\n",
    "    (h, w) = image.shape[:2]\n",
    "    moments = cv2.moments(image)\n",
    "    \n",
    "    # deskew the image by applying an affine transformation\n",
    "    skew = moments[\"mu11\"] / moments[\"mu02\"]\n",
    "    M = np.float32([\n",
    "        [1, skew, -0.5 * w * skew],\n",
    "        [0, 1, 0]])\n",
    "    image = cv2.warpAffine(image, M, (w, h),\n",
    "        flags = cv2.WARP_INVERSE_MAP | cv2.INTER_LINEAR)\n",
    "\n",
    "    # resize the image to have a constant width\n",
    "    image = cv2.resize(image, (28,28))\n",
    "    \n",
    "    # return the deskewed image\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see how a deskew image looks like;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'deskew' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-91a92dcf4053>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdeskew\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtight_layout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'gray'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'none'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'deskew' is not defined"
     ]
    }
   ],
   "source": [
    "image = deskew(data[0],28)\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.tight_layout()\n",
    "plt.imshow(data[0], cmap='gray', interpolation='none')\n",
    "plt.title(\"Skewed Image\")\n",
    "plt.subplot(1,2,2)\n",
    "plt.tight_layout()\n",
    "plt.imshow(image, cmap='gray', interpolation='none')\n",
    "plt.title(\"De-Skewed Image\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets see effect of Extent Center; Code for that will look like;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import mahotas\n",
    "def center_extent(image, size):\n",
    "    # grab the extent width and height\n",
    "    (eW, eH) = size\n",
    "    \n",
    "    #Image Shape is\n",
    "    (h, w) = image.shape[:2]\n",
    "    \n",
    "    #New dimension according to image aspect ratio\n",
    "    dim = None\n",
    "    \n",
    "    # handle when the width is greater than the height\n",
    "    if image.shape[1] > image.shape[0]:\n",
    "        #image = resize(image, width = eW)\n",
    "        r = eW / float(w)\n",
    "        dim = (eW, int(h * r))\n",
    "        image = cv2.resize(image,dim,cv2.INTER_AREA)\n",
    "\n",
    "    # otherwise, the height is greater than the width\n",
    "    else:\n",
    "        #image = resize(image, height = eH)\n",
    "        r = eH / float(h)\n",
    "        dim = (int(w * r), eH)\n",
    "        image = cv2.resize(image,dim,cv2.INTER_AREA)\n",
    "\n",
    "    # allocate memory for the extent of the image and\n",
    "    # grab it\n",
    "    extent = np.zeros((eH, eW), dtype = \"uint8\")\n",
    "    offsetX = (eW - image.shape[1]) / 2\n",
    "    offsetY = (eH - image.shape[0]) / 2\n",
    "    extent[offsetY:offsetY + image.shape[0], offsetX:offsetX + image.shape[1]] = image\n",
    "\n",
    "    # compute the center of mass of the image and then\n",
    "    # move the center of mass to the center of the image\n",
    "    (cY, cX) = np.round(mahotas.center_of_mass(extent)).astype(\"int32\")\n",
    "    (dX, dY) = ((size[0] / 2) - cX, (size[1] / 2) - cY)\n",
    "    M = np.float32([[1, 0, dX], [0, 1, dY]])\n",
    "    extent = cv2.warpAffine(extent, M, size)\n",
    "\n",
    "    # return the extent of the image\n",
    "    return extent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see how it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "im = cv2.imread('Decenter.png')\n",
    "im = cv2.cvtColor(im,cv2.COLOR_BGR2GRAY)\n",
    "image = center_extent(im,(28,28))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.tight_layout()\n",
    "plt.imshow(im, cmap='gray', interpolation='none')\n",
    "plt.title(\"De-Centered\")\n",
    "plt.subplot(1,2,2)\n",
    "plt.tight_layout()\n",
    "plt.imshow(image, cmap='gray', interpolation='none')\n",
    "plt.title(\"Centered Image\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we are ready with our pre-processing modules; now our next task is to extract meaniningfull features out of our images so we can use those features for our use. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Histogram of Oriented Gradients (HOG) features.\n",
    "\n",
    "As its name suggests it contains three key terms\n",
    "\n",
    "- Histogram\n",
    "- Oriented\n",
    "- Gradients\n",
    "\n",
    "Now Histogram is nothing but a frequncy map which shows how many times a random variable appears in the context; Orientation is directly associated with angles; and Gradients signifies transitions of a random variable. so HOG shows us a frequency map of edges(Gradients) in different orientations of an Image.\n",
    "\n",
    "Lets try to understand it with an example\n",
    "\n",
    "Suppose we are having an Image of containing different shapes like following \n",
    "\n",
    "<img src=\"HOG_1.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how we can calculate such histograms for our images;  in practice when we calculate HOG of an image it will always calculated in following manner.\n",
    "\n",
    "<img src=\"HOG_2.png\">\n",
    "\n",
    "So How we will gonna do it in our case, well there is a function in skimage library which allows us to extract HOG features out of the images and we will do the same for our images too; we will write a method fo calculating HOG for our images.\n",
    "\n",
    "definition will go as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from skimage import feature\n",
    "\n",
    "def HOG_describe(image):\n",
    "    # compute HOG for the image\n",
    "    hist = feature.hog(image, orientations = 9,\n",
    "        pixels_per_cell = (8, 8),\n",
    "        cells_per_block = (3, 3)\n",
    "        )\n",
    "    return hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try out our function for an image to calculate its HOG features;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hist = HOG_describe(data[0])\n",
    "\n",
    "print(np.shape(hist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So As you can see there are 81 features were extracted from the image it is roughly 9 histograms from an image with 9 bins "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this is the time to combine all the knowledge we have gained and create a digit classifier so lets roll it we will do following steps to complete this task:\n",
    "\n",
    "- Build a data set\n",
    "- Pre-Process the data set (De-skew and Centralisation)\n",
    "- Train a classifier on the data set.\n",
    "\n",
    "Following method will do the same for us;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "import cPickle\n",
    "def train():\n",
    "    # load the dataset and initialize the data matrix\n",
    "    path2model = 'svm_cs.cpickle'\n",
    "    path2data = 'digits.csv'\n",
    "    \n",
    "    #Image size\n",
    "    factor = 28\n",
    "    (digits, target) = load_digits(path2data)\n",
    "    data = [] \n",
    "    \n",
    "    # loop over the images\n",
    "    for image in digits:\n",
    "        # deskew the image, center it\n",
    "        image = deskew(image, factor)\n",
    "        image = center_extent(image, (factor, factor))\n",
    "    \n",
    "        # describe the image and update the data matrix\n",
    "        hist = HOG_describe(image)\n",
    "        data.append(hist)\n",
    "    \n",
    "    # train the model\n",
    "    model = LinearSVC()\n",
    "    print(model)\n",
    "    model.fit(data, target)\n",
    "    \n",
    "    # dump the model to file\n",
    "    f = open(path2model, \"w\")\n",
    "    f.write(cPickle.dumps(model))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets call the above method for our data set and train a SVM classifier;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So all good till here it is the time to test our classifier on real images\n",
    "lets roll it;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "    \n",
    "    path2model = 'svm_cs.cpickle'\n",
    "    path2im =  'cellphone.png'\n",
    "    factor = 28\n",
    "    # load the model\n",
    "    model = open(path2model).read()\n",
    "    model = cPickle.loads(model)\n",
    "    \n",
    "    # initialize the HOG descriptor\n",
    "    # load the image and convert it to grayscale\n",
    "    image = cv2.imread(path2im)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # blur the image, find edges, and then find contours along\n",
    "    # the edged regions\n",
    "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "    edged = cv2.Canny(blurred, 30, 150)\n",
    "    (_,cnts, _) = cv2.findContours(edged.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # sort the contours by their x-axis position, ensuring\n",
    "    # that we read the numbers from left to right\n",
    "    cnts = sorted([(c, cv2.boundingRect(c)[0]) for c in cnts], key = lambda x: x[1])\n",
    "    \n",
    "    # loop over the contours\n",
    "    for (c, _) in cnts:\n",
    "        # compute the bounding box for the rectangle\n",
    "        (x, y, w, h) = cv2.boundingRect(c)\n",
    "    \n",
    "        # if the width is at least 7 pixels and the height\n",
    "        # is at least 20 pixels, the contour is likely a digit\n",
    "        if w >= 7 and h >= 20:\n",
    "            # crop the ROI and then threshold the grayscale\n",
    "            # ROI to reveal the digit\n",
    "            roi = gray[y:y + h, x:x + w]\n",
    "            thresh = roi.copy()\n",
    "            T = mahotas.thresholding.otsu(roi)\n",
    "            thresh[thresh > T] = 255\n",
    "            thresh = cv2.bitwise_not(thresh)\n",
    "    \n",
    "            # deskew the image center its extent\n",
    "            thresh = deskew(thresh, factor)\n",
    "            thresh = center_extent(thresh, (factor, factor))\n",
    "#             thresh = cv2.resize(thresh,(28,28))    \n",
    "            # extract features from the image and classify it\n",
    "            hist = HOG_describe(thresh)\n",
    "            digit = model.predict(np.array([hist]))[0]\n",
    "            # draw a rectangle around the digit, the show what the\n",
    "            # digit was classified as\n",
    "            cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 1)\n",
    "            cv2.putText(image, str(digit), (x - 10, y - 10),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 2)\n",
    "    plt.figure(figsize=(15,10))\n",
    "    plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cPickle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ea594c21b25d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-d6e88bf66536>\u001b[0m in \u001b[0;36mtest\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[1;31m# load the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath2model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcPickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[1;31m# initialize the HOG descriptor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cPickle' is not defined"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "nbpresent": {
   "slides": {
    "4489ce04-483d-4a1b-9106-8173208a3c7c": {
     "id": "4489ce04-483d-4a1b-9106-8173208a3c7c",
     "prev": null,
     "regions": {
      "7523a0e5-84d5-4ea8-b0b5-328375fdfa99": {
       "attrs": {
        "height": 0.8,
        "width": 0.45,
        "x": 0.5,
        "y": 0.1
       },
       "id": "7523a0e5-84d5-4ea8-b0b5-328375fdfa99"
      },
      "8bb669f8-9725-4e45-9aa7-93948fefae7b": {
       "attrs": {
        "height": 0.8,
        "width": 0.45,
        "x": 0.15000000000000002,
        "y": 0.1
       },
       "id": "8bb669f8-9725-4e45-9aa7-93948fefae7b"
      }
     }
    }
   },
   "themes": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
